# Tutorial 07: LLM Integration with Noodle v3

## ğŸ¯ Learning Objectives

After completing this tutorial, you will:
- âœ… Understand LLM integration in NIP v3
- âœ… Configure Z.ai GLM-4.7 as primary provider
- âœ… Set up fallback providers (OpenAI, Anthropic)
- âœ… Generate patches with AI
- âœ… Track costs and manage budgets
- âœ… Handle LLM errors gracefully

**Prerequisites:**
- Completed Tutorial 06 (A/B Testing)
- API key for Z.ai (or compatible provider)
- Understanding of LLM fundamentals

**Estimated Time:** 60-75 minutes

---

## ğŸ“š What is LLM Integration?

### The Problem: Manual Patch Generation

Writing optimization patches manually is:
- ğŸŒ **Time-consuming:** Each patch takes hours
- ğŸ˜« **Error-prone:** Humans make mistakes
- ğŸ² **Inconsistent:** Quality varies
- ğŸ“š **Knowledge-intensive:** Requires expertise

### The Solution: AI-Powered Patch Generation

NIP v3 uses **Large Language Models** to generate patches automatically:

```python
from noodlecore.improve.llm_integration import LLMManager

llm = LLMManager()
result = llm.generate_patch(task, context)

print(f"ğŸ¤– AI-Generated Patch:")
print(result.patch)
print(f"âœ… Confidence: {result.confidence:.2%}")
print(f"ğŸ’° Cost: ${result.metadata['cost_usd']:.4f}")
```

**Benefits:**
- âš¡ **Fast:** Patches in seconds
- ğŸ¯ **Consistent:** Quality control
- ğŸ’¡ **Creative:** Novel approaches
- ğŸŒ **Multilingual:** Z.ai supports Chinese + English

---

## ğŸ”§ How It Works: Provider Architecture

### Primary Provider: Z.ai GLM-4.7

```python
class ZAIProvider(LLMProvider):
    """
    Z.ai GLM-4.7 - Bilingual, optimized for code
    - Input: ~$0.70 per 1M tokens
    - Output: ~$2.00 per 1M tokens
    - Context: 128K tokens
    """
    def generate_patch(self, task, context):
        # Call Z.ai API
        response = self.api.call(
            model="glm-4.7",
            messages=self._build_prompt(task, context),
            temperature=0.3
        )
        return self._parse_response(response)
```

### Fallback Providers

```python
llm = LLMManager(providers=[
    ZAIProvider(api_key=os.getenv("ZAI_API_KEY")),
    OpenAIProvider(api_key=os.getenv("OPENAI_API_KEY")),
    AnthropicProvider(api_key=os.getenv("ANTHROPIC_API_KEY"))
])

result = llm.generate_patch(task, context)

# If Z.ai fails, automatically falls back to OpenAI
# If OpenAI fails, automatically falls back to Anthropic
```

---

## ğŸš€ Quick Start: Your First AI-Generated Patch

### Step 1: Configure LLM Integration

Update `noodle.json`:

```json
{
  "improve": {
    "llmIntegrationEnabled": true,
    "llm": {
      "provider": "z_ai",
      "model": "glm-4.7",
      "temperature": 0.3,
      "maxTokens": 4096,
      "apiBase": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
      "fallback": [
        {"provider": "openai", "model": "gpt-4-turbo"},
        {"provider": "anthropic", "model": "claude-3-sonnet"}
      ],
      "maxCostPerTask": 1.0,
      "dailyBudget": 10.0,
      "timeoutSeconds": 120
    }
  }
}
```

### Step 2: Set API Keys

```bash
# Z.ai (primary)
export ZAI_API_KEY="your-zai-key"

# OpenAI (fallback)
export OPENAI_API_KEY="your-openai-key"

# Anthropic (fallback)
export ANTHROPIC_API_KEY="your-anthropic-key"
```

### Step 3: Create LLM Patch Generator

Create `examples/llm_patch_example.py`:

```python
from noodlecore.improve.llm_integration import LLMManager
from noodlecore.improve.models import TaskSpec, TaskContext

# Load task
task = TaskSpec.from_json_file("tasks/optimize_function.json")

# Create context
context = TaskContext(
    repository_path=".",
    base_branch="main",
    target_files=["src/performance_critical.py"],
    constraints={
        "max_loc_changed": 100,
        "risk_level": "medium"
    }
)

# Create LLM manager
llm = LLMManager()

# Generate patch
print("ğŸ¤– Generating AI patch...")
result = llm.generate_patch(task, context)

# Display results
print("\nğŸ“‹ Patch Generation Results")
print("=" * 60)
print(f"âœ… Status: {result.status}")
print(f"ğŸ¯ Confidence: {result.confidence:.2%}")
print(f"ğŸ’° Cost: ${result.metadata['cost_usd']:.4f}")
print(f"ğŸ“Š Tokens: {result.metadata['total_tokens']}")
print(f"â±ï¸  Time: {result.metadata['generation_time']:.2f}s")

print(f"\nğŸ¤– AI-Generated Patch:")
print("-" * 60)
print(result.patch)

# Save patch
with open("patches/ai_generated.patch", "w") as f:
    f.write(result.patch)
print("\nâœ… Patch saved to patches/ai_generated.patch")
```

**Expected Output:**
```
ğŸ¤– Generating AI patch...

ğŸ“‹ Patch Generation Results
============================================================
âœ… Status: success
ğŸ¯ Confidence: 87.00%
ğŸ’° Cost: $0.0234
ğŸ“Š Tokens: 1234
â±ï¸  Time: 3.45s

ğŸ¤– AI-Generated Patch:
------------------------------------------------------------
--- a/src/performance_critical.py
+++ b/src/performance_critical.py
@@ -1,6 +1,13 @@
 def process_data(data):
-    result = []
-    for item in data:
-        result.append(transform(item))
-    return result
+    # Optimized with list comprehension
+    return [transform(item) for item in data]

âœ… Patch saved to patches/ai_generated.patch
```

---

## âš™ï¸ Advanced Configuration

### Cost Tracking

Track LLM spending:

```python
from noodlecore.improve.llm_integration import BudgetTracker

tracker = BudgetTracker(daily_budget=10.0)

# Check budget
remaining = tracker.get_remaining_budget()
print(f"ğŸ’° Remaining budget: ${remaining:.2f}")

# Generate patch
result = llm.generate_patch(task, context)

# Log cost
tracker.log_cost(result.metadata['cost_usd'])
print(f"ğŸ’° Cost this task: ${result.metadata['cost_usd']:.4f}")
print(f"ğŸ’° Total today: ${tracker.get_daily_spend():.2f}")
```

### Prompt Engineering

Customize prompts for better results:

```python
from noodlecore.improve.llm_integration import LLMManager

llm = LLMManager()

# Custom prompt template
custom_prompt = """
You are an expert code optimizer. Your task is to:
1. Analyze the code for performance bottlenecks
2. Propose optimizations that:
   - Reduce execution time by at least 20%
   - Maintain code readability
   - Add explanatory comments
3. Consider edge cases and error handling

Code to optimize:
{code}

Constraints:
{constraints}
"""

result = llm.generate_patch(
    task=task,
    context=context,
    prompt_template=custom_prompt
)
```

### Temperature Tuning

Control creativity:

```python
# Low temperature = more deterministic
result = llm.generate_patch(task, context, temperature=0.1)

# Medium temperature = balanced (default)
result = llm.generate_patch(task, context, temperature=0.3)

# High temperature = more creative
result = llm.generate_patch(task, context, temperature=0.7)
```

---

## ğŸ¯ Best Practices

### 1. Use Cost Limits

```python
# âœ… GOOD: Set cost limits
llm = LLMManager(max_cost_per_task=1.0)

try:
    result = llm.generate_patch(task, context)
except CostExceededError:
    print("âš ï¸  Cost limit exceeded, falling back to template")
```

### 2. Handle Timeouts

```python
# âœ… GOOD: Set timeouts
llm = LLMManager(timeout=120)

try:
    result = llm.generate_patch(task, context)
except TimeoutError:
    print("âš ï¸  LLM timeout, using fallback provider")
```

### 3. Validate AI Outputs

```python
# âœ… GOOD: Always validate
result = llm.generate_patch(task, context)

# Validate syntax
if not is_valid_syntax(result.patch):
    print("âŒ Invalid syntax, rejecting")
    return None

# Validate constraints
if exceeds_max_loc(result.patch, task.max_loc_changed):
    print("âŒ Exceeds LOC limit, rejecting")
    return None

print("âœ… AI patch validated")
```

---

## ğŸ› Common Mistakes

### Mistake 1: No API Key

**Problem:** Missing or invalid API key

```python
# âŒ BAD: No API key
# export ZAI_API_KEY=""  # Empty!
```

**Solution:** Always set valid API key

```bash
# âœ… GOOD: Valid API key
export ZAI_API_KEY="sk-..."
```

### Mistake 2: Ignoring Costs

**Problem:** Uncontrolled LLM spending

```python
# âŒ BAD: No cost tracking
result = llm.generate_patch(task, context)
# How much did this cost?
```

**Solution:** Track costs

```python
# âœ… GOOD: Track costs
result = llm.generate_patch(task, context)
print(f"Cost: ${result.metadata['cost_usd']:.4f}")
```

### Mistake 3: Blind Trust

**Problem:** Deploying AI patches without review

```python
# âŒ BAD: No review
result = llm.generate_patch(task, context)
apply_patch(result.patch)  # Dangerous!
```

**Solution:** Always review

```python
# âœ… GOOD: Manual review
result = llm.generate_patch(task, context)
if manual_review(result.patch):
    apply_patch(result.patch)
```

---

## âœ… Exercise: Generate AI Patch

Your turn! Generate an AI-optimized patch.

**Scenario:** Optimize a database query function

**Task:**
1. Configure LLM integration
2. Generate patch with AI
3. Validate and review
4. Apply if approved

**Template:**

```python
# TODO: Implement this
from noodlecore.improve.llm_integration import LLMManager

# TODO: Create LLM manager
llm = LLMManager()

# TODO: Generate patch
result = llm.generate_patch(task, context)

# TODO: Validate
print(f"Confidence: {result.confidence:.2%}")
print(f"Cost: ${result.metadata['cost_usd']:.4f}")

# TODO: Review and apply
if result.confidence > 0.8:
    print("âœ… High confidence, applying patch")
    apply_patch(result.patch)
else:
    print("âš ï¸  Low confidence, manual review needed")
```

---

## ğŸ“ Summary

In this tutorial, you learned:

âœ… **LLM integration** enables automatic patch generation  
âœ… **Z.ai GLM-4.7** is the primary provider (cost-effective)  
âœ… **Fallback providers** ensure reliability  
âœ… **Cost tracking** prevents budget overruns  
âœ… **Validation** is essential for AI outputs  

**Key Takeaways:**
- ğŸ¤– LLMs can **generate patches in seconds**
- ğŸ’° **Cost tracking** is essential (~$0.001-0.01 per patch)
- ğŸ¯ **Confidence scores** help decision-making
- âœ… Always **validate** AI-generated code
- ğŸ”„ **Fallback providers** ensure reliability

---

## ğŸš€ Next Steps

- **Tutorial 08:** LSP Gate
- **Tutorial 09:** Automatic Rollback
- **Tutorial 10:** Analytics Dashboard

**Continue Learning:** [Tutorial 08: LSP Gate](./08_lsp_gate.md)

---

**ğŸ¤– Let AI improve your code with Noodle!**

Questions? [Open an Issue](https://github.com/cubic461/Noodle-Core/issues)
