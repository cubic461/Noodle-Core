#!/usr/bin/env python3
"""
Staged Execution Simulator - Phase 2 Demo

This script demonstrates how to use the Execution Planner and Staged Simulator
to create and test partition plans based on Fase 1 metrics.

Usage:
    python simulate_staged.py --plan balanced --num-nodes 3
    
Requirements:
    - Fase 1 metrics JSONL file (generated by profile_gpt2.py)
"""

import argparse
import sys
import json
from pathlib import Path
import time

from src.metrics import MetricsCollector, LayerMetrics
from src.planner import ExecutionPlanner, PlanningConstraints, PartitionStrategy
from src.plan import VirtualNode, DeviceType, PartitionPlan


def load_metrics_from_jsonl(jsonl_path: Path) -> MetricsCollector:
    """Load metrics from Fase 1 JSONL output."""
    collector = MetricsCollector()
    
    print(f"ðŸ“‚ Loading metrics from: {jsonl_path}")
    
    with open(jsonl_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            
            # Create LayerMetrics object
            metrics = LayerMetrics(
                layer_name=data['layer_name'],
                layer_type=data.get('layer_type', 'Unknown'),
                layer_index=data.get('layer_index', 0),
                forward_latency_ms=data.get('forward_latency_ms', 0.0),
                p50_latency_ms=data.get('p50_latency_ms', 0.0),
                p95_latency_ms=data.get('p95_latency_ms', 0.0),
                p99_latency_ms=data.get('p99_latency_ms', 0.0),
                peak_vram_before=data.get('peak_vram_before', 0),
                peak_vram_after=data.get('peak_vram_after', 0),
                peak_ram_before=data.get('peak_ram_before', 0),
                peak_ram_after=data.get('peak_ram_after', 0),
                num_parameters=data.get('num_parameters', 0),
                device=data.get('device', 'cpu'),
                input_shapes=data.get('input_shapes', []),
                output_shapes=data.get('output_shapes', []),
                input_dtypes=data.get('input_dtypes', []),
                output_dtypes=data.get('output_dtypes', []),
            )
            
            # Add to collector history
            if metrics.layer_name not in collector.metrics_history:
                collector.metrics_history[metrics.layer_name] = []
            collector.metrics_history[metrics.layer_name].append(metrics)
    
    print(f"âœ… Loaded {len(collector.metrics_history)} layers")
    
    return collector


def create_sample_nodes(num_gpu: int = 2, num_cpu: int = 1) -> list[VirtualNode]:
    """Create sample virtual nodes for simulation."""
    nodes = []
    
    # High-end GPU
    if num_gpu >= 1:
        nodes.append(VirtualNode(
            node_id="gpu_0",
            device_type=DeviceType.GPU,
            device_name="NVIDIA GeForce RTX 4090",
            vram_gb=24.0,
            ram_gb=64.0,
            compute_score=10.0,
            network_latency_ms=1.0,
            bandwidth_mbps=5000.0,
            supports_fp16=True,
            supports_int8=True,
            priority=10,
            labels={"fast": "true", "vram": "high"}
        ))
    
    # Mid-range GPU
    if num_gpu >= 2:
        nodes.append(VirtualNode(
            node_id="gpu_1",
            device_type=DeviceType.GPU,
            device_name="NVIDIA GeForce RTX 3070",
            vram_gb=8.0,
            ram_gb=32.0,
            compute_score=6.0,
            network_latency_ms=1.0,
            bandwidth_mbps=5000.0,
            supports_fp16=True,
            supports_int8=False,
            priority=8,
            labels={"fast": "true", "vram": "medium"}
        ))
    
    # CPU nodes
    for i in range(num_cpu):
        nodes.append(VirtualNode(
            node_id=f"cpu_{i}",
            device_type=DeviceType.CPU,
            device_name=f"Intel Core i7-12700K {i}",
            vram_gb=0.0,
            ram_gb=64.0,
            compute_score=1.0,
            network_latency_ms=2.0,
            bandwidth_mbps=1000.0,
            supports_fp16=False,
            supports_int8=True,
            priority=5 - i,
            labels={"fast": "false", "ram": "high", "backend": "cpu"}
        ))
    
    return nodes


def demo_planning(metrics_file: Path, strategy: str, num_nodes: int):
    """Demonstrate the complete planning process."""
    
    print("=" * 80)
    print("ðŸš€ NoodleCore Fase 2: Execution Planning Demo")
    print("=" * 80)
    
    # Step 1: Load metrics
    if not metrics_file.exists():
        print(f"âŒ Metrics file not found: {metrics_file}")
        print("   Run Fase 1 profiling first:")
        print(f"   python examples/profile_gpt2.py --model gpt2 --num-samples 50")
        return
    
    collector = load_metrics_from_jsonl(metrics_file)
    
    # Step 2: Create virtual nodes
    num_gpu = min(num_nodes - 1, 2)  # Max 2 GPUs
    num_cpu = max(num_nodes - num_gpu, 1)  # At least 1 CPU
    nodes = create_sample_nodes(num_gpu=num_gpu, num_cpu=num_cpu)
    
    print(f"\nðŸ–¥ï¸  Virtual Nodes ({len(nodes)} total):")
    for node in nodes:
        print(f"   - {node}")
    
    # Step 3: Generate plans with different strategies
    strategies = [PartitionStrategy(s) for s in [strategy]] if strategy != "all" else list(PartitionStrategy)
    
    plans = {}
    planner = ExecutionPlanner(collector)
    
    for strategy_enum in strategies:
        print(f"\n{'=' * 80}")
        print(f"Strategy: {strategy_enum.value.upper()}")
        print(f"{'=' * 80}")
        
        planner.strategy = strategy_enum
        
        # Adjust constraints based on strategy
        constraints = PlanningConstraints()
        if strategy_enum == PartitionStrategy.BOTTLENECK_FIRST:
            constraints.max_stages = len(nodes)  # One stage per node for bottlenecks
        
        planner.constraints = constraints
        
        # Generate plan
        try:
            plan = planner.generate_plan(nodes, model_name="gpt2")
            plans[strategy_enum.value] = plan
            
            # Display plan
            print("\n" + plan.visualize())
            
            # Export to JSON
            output_file = f"data/plans/{strategy_enum.value}_plan.json"
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w') as f:
                f.write(plan.to_json())
            print(f"\nðŸ’¾ Plan saved: {output_file}")
            
        except Exception as e:
            print(f"âŒ Error generating {strategy_enum.value} plan: {e}")
            import traceback
            traceback.print_exc()
    
    # Step 4: Compare strategies (if multiple)
    if len(plans) > 1:
        print(f"\n{'=' * 80}")
        print("ðŸ“Š Strategy Comparison")
        print(f"{'=' * 80}")
        
        print("\nStrategy           | Latency (ms) | Stages | Load Balance | Bottleneck")
        print("-" * 80)
        
        for strategy_name, plan in sorted(plans.items()):
            bottleneck_info = f"Stage {plan.bottleneck_stage_id}" if plan.bottleneck_stage_id is not None else "N/A"
            print(f"{strategy_name:19} | {plan.total_expected_latency_ms:12.1f} | {len(plan.stages):6} | "
                  f"{plan.load_balance_score:12.2f} | {bottleneck_info}")
        
        # Recommend best strategy
        best_latency = min(plans.values(), key=lambda p: p.total_expected_latency_ms)
        best_balanced = max(plans.values(), key=lambda p: p.load_balance_score)
        
        print(f"\nðŸŽ¯ Recommendations:")
        print(f"   Fastest: {best_latency.plan_name.replace('gpt2_', '').replace('_plan', '')} "
              f"({best_latency.total_expected_latency_ms:.0f}ms)")
        print(f"   Most Balanced: {best_balanced.plan_name.replace('gpt2_', '').replace('_plan', '')} "
              f"(score: {best_balanced.load_balance_score:.2f})")
    
    print("\n" + "=" * 80)
    print("âœ… Demo complete!")
    print("=" * 80)
    print("\nNext steps:")
    print("1. Review generated plans in data/plans/")
    print("2. Use plans for distributed inference (Fase 3)")
    print("3. Test with real hardware on your second machine")


def main():
    parser = argparse.ArgumentParser(
        description='NoodleCore Fase 2: Execution Planning Demo',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python examples/simulate_staged.py --plan balanced --num-nodes 3
  python examples/simulate_staged.py --plan all --num-nodes 2
  
Note: Requires Fase 1 metrics file (data/metrics/gpt2_profile_metrics.jsonl)
        """
    )
    
    parser.add_argument(
        '--metrics',
        type=str,
        default='data/metrics/gpt2_profile_metrics.jsonl',
        help='Path to metrics JSONL file (default: data/metrics/gpt2_profile_metrics.jsonl)'
    )
    parser.add_argument(
        '--plan',
        type=str,
        default='balanced',
        choices=['balanced', 'bottleneck_first', 'memory_aware', 'latency_optimized', 'all'],
        help='Planning strategy (default: balanced)'
    )
    parser.add_argument(
        '--num-nodes',
        type=int,
        default=3,
        help='Number of virtual nodes (default: 3)'
    )
    
    args = parser.parse_args()
    
    # Validate
    if args.num_nodes < 1:
        print("âŒ Error: --num-nodes must be at least 1")
        sys.exit(1)
    
    # Run demo
    metrics_file = Path(args.metrics)
    
    try:
        demo_planning(metrics_file, args.plan, args.num_nodes)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
